# Gradient Descent and Momentum-Based Gradient Descent Tutorial

## Introduction
Welcome to our bonus tutorial where you will dive deep into the fundamentals of Gradient Descent, understand its limitations, and explore its applications in neural networks. This tutorial is designed to enhance your understanding of one of the most powerful optimization techniques in machine learning.

### What You Will Learn
- **Gradient Descent**: You'll learn about Gradient Descent, a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
- **Limitations of Gradient Descent**: We'll discuss some of the challenges and limitations associated with the basic Gradient Descent algorithm.
- **Application in Neural Networks**: Understand how Gradient Descent is used to train neural networks by iteratively improving model parameters to minimize the loss function.
- **Momentum-Based Gradient Descent**: We will explore a variation of Gradient Descent, known as Momentum-Based Gradient Descent, which helps in accelerating the convergence towards the global minimum and stabilizing the updates.

## Project Objective
The primary objective of this bonus is to implement the Gradient Descent algorithm in a way that a neural network learns to approximate the sine function. This hands-on experience will not only solidify your understanding of the theoretical concepts but also enhance your practical skills in implementing neural networks.

## Resources Provided
- **Jupyter Notebook**: A Jupyter notebook named `neural_net_gradient_descent.ipynb` is provided, which contains step-by-step instructions and code cells to guide you through the implementation of both basic and Momentum-Based Gradient Descent in a neural network setting.
- **Python Script**: For those who prefer a more traditional coding approach, a Python script is available upon request where you can implement the same functionality.

## Getting Started
1. **Clone the Repository**: Begin by cloning this repository to your local machine to access the Jupyter notebook and other resources.
2. **Install Dependencies**: Ensure that you have Python installed, along with libraries such as NumPy and Matplotlib, to run the notebook and scripts.
3. **Explore the Notebook**: Open the `neural_net_gradient_descent.ipynb` notebook in Jupyter and follow the instructions to learn about and implement the algorithms.

## Conclusion
This tutorial is designed not just to teach you about Gradient Descent and its variations but also to provide a practical implementation scenario where you can apply these concepts directly. By the end of this tutorial, you should have a solid understanding of how Gradient Descent functions, its significance in neural networks, and how variations like Momentum-Based Gradient Descent can overcome some of its limitations.

Enjoy your learning journey!